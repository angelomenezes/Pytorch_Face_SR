{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import glob as gb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "model_path = '/home/angelo/Desktop/Github/Feature-Extractors/'\n",
    "os.sys.path.append(model_path)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing upsampling via SR\n",
    "class SuperResolveDL:\n",
    "    def __init__(self, model, size, channel, resize=True):\n",
    "        self.model = model\n",
    "        self.size = size\n",
    "        self.channel = channel\n",
    "        self.resize = resize\n",
    "        self.VGGmean = (131.0912, 103.8827, 91.4953)\n",
    "\n",
    "    def __call__(self, path=''):\n",
    "        if self.channel == 1:\n",
    "            img = Image.open(path).convert('YCbCr').resize((self.size,self.size),resample=Image.BICUBIC)\n",
    "            upsample, cb, cr = img.split()\n",
    "            if not self.resize:\n",
    "                upsample, _, _ = Image.open(path).convert('YCbCr').split()\n",
    "        elif self.channel == 3:\n",
    "            if self.resize: \n",
    "                upsample = Image.open(path).convert('RGB').resize((self.size,self.size),resample=Image.BICUBIC)\n",
    "            else:\n",
    "                upsample = Image.open(path).convert('RGB')\n",
    "        else:\n",
    "            raise Exception('Channel option not listed')\n",
    "        \n",
    "        data = (transforms.ToTensor()(upsample)).view(1, self.channel, upsample.size[1], upsample.size[0])\n",
    "        data = data.to(device)\n",
    "        out = self.model(data)\n",
    "        out = out.cpu()\n",
    "        out_img = out.data[0].numpy()\n",
    "        out_img *= 255.0\n",
    "        out_img = out_img.clip(0, 255)\n",
    "                \n",
    "        if self.channel == 1:\n",
    "            out_img = Image.fromarray(np.uint8(out_img[0]), mode='L')\n",
    "            out_img = Image.merge('YCbCr', [out_img, cb, cr]).convert('RGB')\n",
    "        \n",
    "        if self.channel == 3:\n",
    "            out_img = out_img.transpose((1,2,0))\n",
    "            out_img = Image.fromarray(np.uint8(out_img), mode='RGB')\n",
    "\n",
    "        return np.array(out_img) - self.VGGmean\n",
    "    \n",
    "    \n",
    "# Testing upsampling via Bicubic\n",
    "\n",
    "def load_image(path='', size=224, resize=True): # Bicubic\n",
    "    '''\n",
    "    VGGFace2 Data has mean = (131.0912, 103.8827, 91.4953)\n",
    "    and std = (1,1,1) with no scaling\n",
    "    '''\n",
    "    img = Image.open(path)\n",
    "    mean = (131.0912, 103.8827, 91.4953)\n",
    "    if resize:\n",
    "        img = img.resize((size,size), resample=Image.BICUBIC)\n",
    "    img = (np.array(img) - mean) # Normalizing data before inference according to VGG parameters\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of models\n",
    "\n",
    "def initialize_senet50_2048():\n",
    "    import senet50_ft_pytorch.senet50_ft_dims_2048 as model\n",
    "    network = model.senet50_ft(weights_path=model_path + 'senet50_ft_pytorch/senet50_ft_dims_2048.pth')\n",
    "    network.eval()\n",
    "    return network\n",
    "\n",
    "def initialize_senet50_256():\n",
    "    import senet50_256_pytorch.senet50_256 as model\n",
    "    network = model.senet50_256(weights_path=model_path + 'senet50_256_pytorch/senet50_256.pth')\n",
    "    network.eval()\n",
    "    return network\n",
    "\n",
    "def initialize_senet50_128():\n",
    "    import senet50_128_pytorch.senet50_128 as model\n",
    "    network = model.senet50_128(weights_path=model_path + 'senet50_128_pytorch/senet50_128.pth')\n",
    "    network.eval()\n",
    "    return network\n",
    "\n",
    "def initialize_resnet50_128():\n",
    "    import resnet50_128_pytorch.resnet50_128 as model\n",
    "    network = model.resnet50_128(weights_path=model_path + 'resnet50_128_pytorch/resnet50_128.pth')\n",
    "    network.eval()\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    return 1 - dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def euc_dis(a,b):\n",
    "    return norm(a-b)\n",
    "\n",
    "def l2_error(a,b):\n",
    "    return sum((a-b)**2) \n",
    "\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "'''\n",
    "def return_batch_embedding(model, facepaths, embedding_size=256):\n",
    "    batch_size = 10 # As original implementation suggests\n",
    "    num_faces = len(facepaths)\n",
    "    face_feats = np.empty((num_faces, embedding_size))\n",
    "    imgchunks = list(chunks(facepaths, batch_size))\n",
    "\n",
    "    for c, imgs in enumerate(imgchunks):\n",
    "        im_array = np.array([load_image(i) for i in imgs])\n",
    "        f = model(torch.Tensor(im_array.transpose(0, 3, 1, 2)))[1].detach().cpu().numpy()[:, :, 0, 0]\n",
    "        start = c * batch_size\n",
    "        end = min((c + 1) * batch_size, num_faces)\n",
    "        # This is different from the Keras model where the normalization has been done inside the model.\n",
    "        face_feats[start:end] = f / np.sqrt(np.sum(f ** 2, -1, keepdims=True))\n",
    "    \n",
    "    return face_feats\n",
    "'''\n",
    "def return_batch_embedding(model, facepaths, embedding_size=256):\n",
    "    emb_array = [return_embedding(model, face) for face in facepaths]\n",
    "    return np.array(emb_array)\n",
    "\n",
    "def return_embedding(model, img_path):\n",
    "    im_array = load_image(img_path).reshape((1,3,224,224))\n",
    "    f = model(torch.Tensor(im_array))[1].detach().cpu().numpy()[:, :, 0, 0]\n",
    "    face_feats = f / np.sqrt(np.sum(f ** 2, -1, keepdims=True)) # Normalizing embedding\n",
    "    return face_feats.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Loss\n",
      "0.01939290761947632\n"
     ]
    }
   ],
   "source": [
    "# Testing consistency of embedding\n",
    "\n",
    "#senet50 = initialize_senet50_2048()\n",
    "senet50 = initialize_senet50_128()\n",
    "\n",
    "low_res_img_path = '/media/angelo/DATEN/Datasets/CelebA/LR_56/test/018510.jpg'\n",
    "original_img_path = '/media/angelo/DATEN/Datasets/CelebA/HR/test/018510.jpg'\n",
    "\n",
    "lr_emb = return_embedding(senet50, low_res_img_path)\n",
    "hr_emb = return_embedding(senet50, original_img_path)\n",
    "\n",
    "print('Identity Loss')\n",
    "print(cos_sim(lr_emb, hr_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentation\n",
    "\n",
    "img_path = '/media/angelo/DATEN/Datasets/ICB-RW/Margin1.3_112/'\n",
    "\n",
    "gallery_path = gb.glob(img_path + 'gallery/*.jpg')\n",
    "gallery_names = [str(int(os.path.splitext(name)[0][:-2])) for name in os.listdir(img_path + 'gallery/')]\n",
    "\n",
    "face_embeddings = return_batch_embedding(senet50, gallery_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For student 131\n",
      "Similarity with his own probe photos: \n",
      "[0.15156465768814087, 0.18589234352111816, 0.13159418106079102, 0.07728713750839233, 0.1742457151412964]\n",
      "Similarity with probe photos of 3\n",
      "[0.11547565460205078, 0.08119481801986694, 0.20344936847686768, 0.07036924362182617, 0.10211122035980225]\n",
      "Similarity with probe photos of 100\n",
      "[0.0982743501663208, 0.14454597234725952, 0.039524853229522705, 0.07082724571228027, 0.062441468238830566]\n",
      "Similarity with probe photos of 101\n",
      "[0.16037756204605103, 0.11263895034790039, 0.04568207263946533, 0.06749409437179565, 0.058856308460235596]\n"
     ]
    }
   ],
   "source": [
    "probe_path = img_path + 'probe/'\n",
    "student_pool_list = gallery_names\n",
    "\n",
    "real_embedding = return_embedding(senet50, img_path + 'gallery/00' + student_pool_list[0] + '_f.jpg')\n",
    "\n",
    "guilty_student_path = gb.glob(probe_path + student_pool_list[0] + '/*.jpg')\n",
    "sample_1_student = gb.glob(probe_path + student_pool_list[1] + '/*.jpg')\n",
    "sample_2_student = gb.glob(probe_path + student_pool_list[2] + '/*.jpg')\n",
    "sample_3_student = gb.glob(probe_path + student_pool_list[3] + '/*.jpg')\n",
    "\n",
    "print('For student ' + student_pool_list[0])\n",
    "print('Similarity with his own probe photos: ')\n",
    "\n",
    "guilty_student_emb = return_batch_embedding(senet50, guilty_student_path)\n",
    "similarities = [cos_sim(real_embedding, guilty_student_emb[i]) for i in range(5)]\n",
    "print(similarities)\n",
    "\n",
    "print('Similarity with probe photos of ' + student_pool_list[1])\n",
    "\n",
    "sample_1_emb = return_batch_embedding(senet50, sample_1_student)\n",
    "similarities = [cos_sim(real_embedding, sample_1_emb[i]) for i in range(5)]\n",
    "print(similarities)\n",
    "\n",
    "print('Similarity with probe photos of ' + student_pool_list[2])\n",
    "\n",
    "sample_2_emb = return_batch_embedding(senet50, sample_2_student)\n",
    "similarities = [cos_sim(real_embedding, sample_2_emb[i]) for i in range(5)]\n",
    "print(similarities)\n",
    "\n",
    "print('Similarity with probe photos of ' + student_pool_list[3])\n",
    "\n",
    "sample_3_emb = return_batch_embedding(senet50, sample_3_student)\n",
    "similarities = [cos_sim(real_embedding, sample_3_emb[i]) for i in range(5)]\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20\n",
      "Error happend with the following students: \n",
      "['131', '3', '100', '101', '104', '108', '109', '10', '112', '113', '115', '117', '118', '11', '121', '123', '128', '12', '42', '45', '46', '49', '4', '55', '57', '59', '5', '6', '70', '75', '7', '80', '81', '8', '92', '93', '94', '96', '98', '9', '134', '135', '138', '139', '13', '145', '148', '149', '14', '151', '159', '15', '162', '165', '167', '16', '171', '172', '175', '177', '179', '181', '188', '18', '19', '24', '25', '27', '29', '2', '33', '34']\n"
     ]
    }
   ],
   "source": [
    "# Experimentation\n",
    "probe_path = img_path + 'probe/'\n",
    "student_pool_list = gallery_names\n",
    "accuracy = 0\n",
    "error = []\n",
    "\n",
    "for index, student in enumerate(student_pool_list):\n",
    "    probe_list_students = student_pool_list.copy()\n",
    "    watch_list_emb = []\n",
    "    watch_list_images = []\n",
    "    \n",
    "    # First embeddings is always for the guilty student  \n",
    "    guilty_student_image = probe_path + student + '/' + random.choice(os.listdir(probe_path + student))\n",
    "    watch_list_emb.append(return_embedding(senet50,guilty_student_image))\n",
    "    \n",
    "    probe_list_students.remove(student)\n",
    "    \n",
    "    watch_list_students = random.sample(probe_list_students, 4)\n",
    "    \n",
    "    for suspect in watch_list_students:\n",
    "        image = random.choice(os.listdir(probe_path + suspect))\n",
    "        watch_list_images.append(probe_path + suspect + '/' + image)\n",
    "        \n",
    "    for suspect in watch_list_images:\n",
    "        watch_list_emb.append(return_embedding(senet50,suspect))\n",
    "    \n",
    "    similarities = [cos_sim(face_embeddings[index], watch_list_emb[i]) for i in range(5)]\n",
    "    \n",
    "    # If the least distance is the first element, let's sum 1\n",
    "    if np.argmin(np.array(similarities)) == 0:\n",
    "        accuracy += 1\n",
    "    else:\n",
    "        error.append(student)\n",
    "        #print('Error for student ' + student)\n",
    "        #print(pd.DataFrame(similarities))\n",
    "    #print('Student {}/{}'.format(index+1,len(student_pool_list)))\n",
    "    #print('For student ' + student)\n",
    "    #print(pd.DataFrame(similarities))\n",
    "    #print('------------------------------------------------')\n",
    "              \n",
    "print(\"Accuracy: {0:.2f}\".format(accuracy/len(student_pool_list)))\n",
    "print('Error happend with the following students: ')\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
