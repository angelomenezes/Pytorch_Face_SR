{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from super_resolution_data_loader_GAN import *\n",
    "from pytorch_ssim import *\n",
    "\n",
    "from SRGAN_model import Generator, Discriminator\n",
    "from loss import GeneratorLoss\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRGAN parameters\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "threads = 4\n",
    "upscale_factor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path_low = '/media/angelo/DATEN/Datasets/Experiment_Masters/300W-3D-low-res-56/train'\n",
    "#img_path_ref = '/media/angelo/DATEN/Datasets/Experiment_Masters/300W-3D-low-res-224/train'\n",
    "\n",
    "img_path_low = '../dataset/300W-3D-crap-56/train'\n",
    "img_path_ref = '../dataset/300W-3D-low-res-224/train'\n",
    "\n",
    "train_set = DatasetSuperRes(img_path_low, img_path_ref)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=threads, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generator parameters: 734219\n",
      "# Discriminator parameters: 5215425\n"
     ]
    }
   ],
   "source": [
    "netG = Generator(upscale_factor).to(device)\n",
    "print('# Generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
    "netD = Discriminator().to(device)\n",
    "print('# Discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
    "generator_criterion = GeneratorLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "    \n",
    "results = {'d_loss': [], 'g_loss': [], 'd_score': [],\n",
    "           'g_score': [], 'psnr': [], 'ssim': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, \n",
    "                       'd_score': 0, 'g_score': 0}\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    for data, target in training_data_loader:\n",
    "        #train_bar = tqdm(training_data_loader)\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "        ###########################\n",
    "        real_img = Variable(target).to(device)\n",
    "        z = Variable(data).to(device)\n",
    "        \n",
    "        fake_img = netG(z)\n",
    "\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_out = netD(real_img).mean()\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        \n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        \n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Loss for current batch before optimization \n",
    "\n",
    "        running_results['g_loss'] += g_loss.item() * batch_size\n",
    "        running_results['d_loss'] += d_loss.item() * batch_size\n",
    "        running_results['d_score'] += real_out.item() * batch_size\n",
    "        running_results['g_score'] += fake_out.item() * batch_size\n",
    "\n",
    "    #train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "    #        epoch, num_epochs, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "    #        running_results['g_loss'] / running_results['batch_sizes'],\n",
    "    #        running_results['d_score'] / running_results['batch_sizes'],\n",
    "    #        running_results['g_score'] / running_results['batch_sizes']))\n",
    "\n",
    "    print('[{}/{}] Loss_D: {} Loss_G: {} D(x): {} D(G(z)): {}'.format(\n",
    "            epoch, num_epochs, \n",
    "            running_results['d_loss'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss'] / running_results['batch_sizes'],\n",
    "            running_results['d_score'] / running_results['batch_sizes'],\n",
    "            running_results['g_score'] / running_results['batch_sizes']))\n",
    "\n",
    "    netG.eval()\n",
    "\n",
    "    batch_mse = ((fake_img - real_img) ** 2).data.mean()\n",
    "    batch_ssim = ssim(fake_img, real_img).item()\n",
    "    batch_psnr = 10 * log10(1 /batch_mse)\n",
    "\n",
    "    out_path = 'SRGAN_results/'\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)    \n",
    "\n",
    "    # Save loss\\scores\\psnr\\ssim\n",
    "    results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "    results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "    results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "    results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "    results['psnr'].append(batch_psnr)\n",
    "    results['ssim'].append(batch_ssim)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "\n",
    "        # Save model parameters\n",
    "        torch.save(netG, 'netG_epoch_%d_%d.pth' % (upscale_factor, epoch))\n",
    "        #torch.save(netD, 'netD_epoch_%d_%d.pth' % (upscale_factor, epoch))\n",
    "\n",
    "        data_frame = pd.DataFrame(\n",
    "            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                  'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "            index=range(1, epoch + 1))\n",
    "        data_frame.to_csv(out_path + 'SRGAN_x' + str(upscale_factor) + '_train_results.csv', index_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss_D: 0.9606085551169611 Loss_G: 0.027038411163995343 D(x): 0.25200491034936523 D(G(z)): 0.1966510102080722\n",
      "[2/10] Loss_D: 1.0030325412750245 Loss_G: 0.011348660630324194 D(x): 0.19897432315734126 D(G(z)): 0.19661091198844294\n",
      "[3/10] Loss_D: 1.0007452163388653 Loss_G: 0.009760453354927801 D(x): 0.42396586933443625 D(G(z)): 0.42544204235076905\n",
      "[4/10] Loss_D: 1.000786513051679 Loss_G: 0.008780144039661654 D(x): 0.7122460011513002 D(G(z)): 0.7123295261782985\n",
      "[5/10] Loss_D: 1.0027116019495073 Loss_G: 0.00774245516126675 D(x): 0.7360384329672782 D(G(z)): 0.7347663675585101\n",
      "[6/10] Loss_D: 1.0048047944038145 Loss_G: 0.007435147737062746 D(x): 0.6086329825462834 D(G(z)): 0.6032832504472425\n",
      "[7/10] Loss_D: 0.9987230831576932 Loss_G: 0.007897250700381494 D(x): 0.12551476273895992 D(G(z)): 0.12414137604180724\n",
      "[8/10] Loss_D: 1.0030247536013204 Loss_G: 0.007360470990740484 D(x): 0.29405972267350844 D(G(z)): 0.2989126700355161\n",
      "[9/10] Loss_D: 1.0007373746748893 Loss_G: 0.00712976171484878 D(x): 0.16891504968366317 D(G(z)): 0.1679410974441036\n",
      "[10/10] Loss_D: 1.0010542993391713 Loss_G: 0.006810647357375391 D(x): 0.1726482936259239 D(G(z)): 0.1721208765622108\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
